{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de4b16fb",
   "metadata": {},
   "source": [
    "### Лабораторная работа 4 \n",
    "### Тема: Детектирование движения и отслеживание на видео "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ae418",
   "metadata": {},
   "source": [
    "1. Разработать программу детектирования движения на видео методом \n",
    "вычитания модели фона (Background Subtraction Methods).  \n",
    "При возникновении движения в поле зрения камеры (или на \n",
    "видео) подается сигнал (или выводится сообщение).  \n",
    "Пример из жизни – звуковая сигнализация при обнаружении движения, или \n",
    "владельцу наблюдаемого помещения приходит смс с сообщением."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8ddce",
   "metadata": {},
   "source": [
    "Что такое детекция движения и зачем она нужна\n",
    "Детекция движения — это процесс в компьютерном зрении, когда программа анализирует видео или поток с камеры, чтобы понять, есть ли в кадре что-то движущееся. Это как \"глаза\" для компьютера, которые помогают замечать изменения. Полезно в системах безопасности (например, камеры видеонаблюдения, которые включают тревогу при движении), робототехнике (робот реагирует на приближающихся людей), трафик-контроле (считает машины) или даже в играх и приложениях для фитнеса (отслеживает движения тела). В твоём коде это реализовано просто: программа смотрит на видео, выделяет фон (то, что не меняется), и если что-то меняется — сигнализирует о движении.\n",
    "Теория основана на идее, что в статичной сцене (например, пустая комната) фон остаётся примерно одинаковым, а движение — это отклонения от этого фона. Главный метод здесь — вычитание фона (background subtraction). Это когда из текущего кадра \"вычитают\" модель фона, и то, что остаётся, считается передним планом (foreground), то есть движущимися объектами.\n",
    "Ключевые термины (терминология)\n",
    "Давай разберём простыми словами основные понятия, которые встречаются в коде и теории:\n",
    "\n",
    "Фон (Background): Это статичная часть сцены, которая не меняется со временем. Например, стены, пол, мебель в комнате. Программа учится понимать, что это \"нормально\" и неинтересно.\n",
    "Передний план (Foreground): Движущиеся объекты, которые отличаются от фона. Например, человек, проходящий мимо, или машина на дороге.\n",
    "Маска переднего плана (Foreground Mask или FG Mask): Это чёрно-белое изображение, где белые пиксели — это движение (передний план), а чёрные — фон. В коде это fgmask — как \"карта\" изменений.\n",
    "Вычитание фона (Background Subtraction): Основной алгоритм. Он строит модель фона на основе предыдущих кадров и сравнивает новый кадр с этой моделью. Если пиксель в новом кадре сильно отличается — он помечается как движение.\n",
    "MOG2 (Mixture of Gaussians 2): Это конкретный алгоритм вычитания фона, который используется в коде (cv2.createBackgroundSubtractorMOG2()). \"Mixture of Gaussians\" значит \"смесь гауссовых распределений\" — математическая модель, которая описывает, как могут варьироваться цвета пикселей на фоне (учитывает шум, освещение и т.д.). \"2\" — это улучшенная версия, которая лучше справляется с тенями и изменениями света.\n",
    "Порог (Threshold): Число, которое определяет, когда считать изменение \"движением\". В коде это motion > 1000 — если белых пикселей в маске больше 1000, то движение обнаружено. Это чтобы игнорировать мелкий шум, как дрожание камеры или лёгкие блики.\n",
    "Кадр (Frame): Один снимок из видео. Видео — это последовательность кадров, как слайд-шоу.\n",
    "Пиксель (Pixel): Маленькая точка на изображении, из которых состоит весь кадр. Каждый имеет цвет (RGB — красный, зелёный, синий).\n",
    "Шум (Noise): Нежелательные помехи, как случайные изменения в кадре от плохого освещения или сенсора камеры. Алгоритмы вроде MOG2 пытаются их игнорировать.\n",
    "OpenCV: Библиотека (набор готовых функций) для компьютерного зрения. В коде это cv2 — она делает всю тяжёлую работу, как чтение видео, применение алгоритмов и показ изображений.\n",
    "\n",
    "Как работает метод в коде (пошагово и просто)\n",
    "Твой код использует MOG2 для вычитания фона. Вот как это работает на теории и в практике:\n",
    "\n",
    "\n",
    "Инициализация: Программа открывает видео или камеру (cap = cv2.VideoCapture(0) — 0 значит веб-камера). Создаёт объект MOG2 (fgbg = cv2.createBackgroundSubtractorMOG2()). Этот объект — как \"мозг\", который будет учиться на кадрах.\n",
    "\n",
    "\n",
    "Обработка каждого кадра: В цикле while программа читает кадр за кадром (ret, frame = cap.read()). Если кадр есть, применяет MOG2: fgmask = fgbg.apply(frame).\n",
    "\n",
    "Что делает MOG2 внутри:\n",
    "\n",
    "Для каждого пикселя в кадре MOG2 смотрит на историю предыдущих кадров. Он моделирует фон как смесь нескольких \"гауссов\" — это как нормальные распределения в статистике, где цвета пикселей могут слегка варьироваться (например, из-за света).\n",
    "Если пиксель в новом кадре вписывается в модель фона (похож на то, что было раньше) — он чёрный в маске (фон).\n",
    "Если не вписывается (сильно отличается) — белый (передний план, движение).\n",
    "MOG2 адаптируется: если что-то статично долго (например, стул передвинули и оставили), оно со временем станет частью фона.\n",
    "Плюс, MOG2 обнаруживает тени и игнорирует их, чтобы не путать с реальным движением.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Проверка на движение: Считает белые пиксели в маске (motion = cv2.countNonZero(fgmask)). Если их много (больше 1000) — значит, движение! Добавляет текст на кадр (cv2.putText) и печатает \"Сигнал: движение!\".\n",
    "\n",
    "\n",
    "Визуализация: Показывает оригинальный кадр (cv2.imshow('Frame')) и маску (cv2.imshow('FG Mask')). Ждёт нажатия Esc для выхода.\n",
    "\n",
    "\n",
    "Завершение: Закрывает видео и окна.\n",
    "\n",
    "\n",
    "В целом, это работает как \"детектор изменений\": фон — стабильный, движение — отклонение. Но это не идеально: если фон меняется (например, день/ночь), или объекты двигаются медленно — могут быть ошибки. Порог 1000 — это тюнинг, чтобы избежать ложных срабатываний от шума.\n",
    "Что ещё есть: альтернативы и расширения\n",
    "Кроме MOG2, есть другие способы детекции движения. Я расскажу не слишком глубоко, но подробно, простыми словами:\n",
    "\n",
    "Другие алгоритмы вычитания фона в OpenCV:\n",
    "\n",
    "MOG (первый вариант): Похож на MOG2, но хуже справляется с тенями и изменениями освещения. MOG2 — улучшенная версия.\n",
    "KNN (K-Nearest Neighbors): Ещё один метод в OpenCV (cv2.createBackgroundSubtractorKNN()). Работает на основе \"ближайших соседей\" — сравнивает пиксели с похожими из истории. Лучше для сцен с медленными изменениями, но может быть медленнее MOG2. Можно заменить в коде и сравнить.\n",
    "GMG (Godbehere-Matsukawa-Goldberg): Комбинирует статистики и байесовские модели. Хорош для шумных видео, но требует больше вычислений.\n",
    "\n",
    "\n",
    "Альтернативные подходы без вычитания фона:\n",
    "\n",
    "Оптический поток (Optical Flow): Смотрит, как пиксели \"текут\" между кадрами (как вода). Методы вроде Lucas-Kanade или Farneback в OpenCV. Полезно для отслеживания направления движения, но сложнее и медленнее. Не выделяет фон, а фокусируется на векторах движения.\n",
    "Разница кадров (Frame Differencing): Простой способ — вычитаешь предыдущий кадр из текущего. Дёшево, но не справляется с медленным движением или если камера двигается. В коде можно добавить diff = cv2.absdiff(prev_frame, frame) и thresholding.\n",
    "Контурный анализ: После маски (как в твоём коде) можно найти контуры (cv2.findContours) — границы объектов. Затем фильтровать по размеру, чтобы игнорировать мелкие движения (например, листья на ветру).\n",
    "Машинное обучение: Современные методы, как YOLO или Mask R-CNN, используют нейросети для детекции объектов (не просто движения, а \"это человек\" или \"машина\"). Они точнее, но требуют GPU и обучения. В OpenCV есть интеграция с TensorFlow или PyTorch.\n",
    "\n",
    "\n",
    "Улучшения для твоего кода:\n",
    "\n",
    "Фильтрация шума: После маски применить эрозию/дилятацию (cv2.erode, cv2.dilate) — чтобы убрать мелкие белые точки.\n",
    "Отслеживание объектов: Использовать Kalman Filter или SORT, чтобы следить за одним и тем же объектом через кадры (не просто \"движение\", а \"человек идёт вправо\").\n",
    "Адаптация к изменениям: MOG2 имеет параметры, как history (сколько кадров помнить) или detectShadows (включить/выключить тени). По умолчанию тени детектируются как серые в маске.\n",
    "Проблемы и решения: Если освещение меняется — используй адаптивные пороги. Для движущейся камеры (дрон) — нужны более сложные методы, как SLAM.\n",
    "\n",
    "\n",
    "\n",
    "В общем, MOG2 — хороший старт для простых задач, но для реальных приложений комбинируют несколько методов. Если хочешь, могу помочь доработать код или объяснить что-то конкретнее!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de996a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall opencv-python opencv-contrib-python -y\n",
    "%pip install opencv-python numpy\n",
    "import cv2\n",
    "\n",
    "# Видео можно заменить на 0 для веб-камеры\n",
    "cap = cv2.VideoCapture(0)  # Замените на путь к своему видео или 0 для веб-камеры\n",
    "\n",
    "# Создание объекта для вычитания фона (MOG2 наиболее часто используется)\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "iter = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Определение, есть ли движение (по наличию белых пикселей)\n",
    "    motion = cv2.countNonZero(fgmask)\n",
    "    if motion > 1000:  # Порог, чтобы не реагировать на шум\n",
    "\n",
    "        cv2.putText(frame, f\"Detected motion: {iter}\", (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "        iter += 1 \n",
    "        print('Сигнал: движение!')\n",
    "\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('FG Mask', fgmask)\n",
    "    if cv2.waitKey(30) & 0xFF == 27:  # Esc для выхода\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f9f44",
   "metadata": {},
   "source": [
    "Что улучшено в коде\n",
    "\n",
    "Проверка открытия камеры: Добавлена проверка cap.isOpened() в начале, чтобы сразу сообщить об ошибке, если камера недоступна.\n",
    "Настройка MOG2:\n",
    "\n",
    "history=500: Увеличено количество кадров для модели фона, чтобы лучше адаптироваться к изменениям.\n",
    "varThreshold=16: Порог для классификации пикселей (фон/передний план) оставлен стандартным, но можно настроить для чувствительности.\n",
    "detectShadows=True: Тени отображаются серым, что помогает отличать их от реальных объектов.\n",
    "\n",
    "\n",
    "Фильтрация шума:\n",
    "\n",
    "Добавлены морфологические операции (cv2.morphologyEx):\n",
    "\n",
    "MORPH_OPEN: Удаляет мелкие белые точки (шум) в маске.\n",
    "MORPH_CLOSE: Заполняет мелкие разрывы в движущихся объектах.\n",
    "\n",
    "\n",
    "Используется эллиптическое ядро (cv2.getStructuringElement) размером 5x5 для сглаживания.\n",
    "\n",
    "\n",
    "Контурный анализ:\n",
    "\n",
    "Вместо подсчёта белых пикселей (cv2.countNonZero) добавлен поиск контуров (cv2.findContours).\n",
    "Контуры с площадью меньше min_contour_area (500 пикселей) игнорируются, чтобы исключить шум.\n",
    "Для каждого значимого контура рисуется зелёный прямоугольник (cv2.boundingRect), что визуально показывает, где именно обнаружено движение.\n",
    "\n",
    "\n",
    "Улучшенная логика детекции:\n",
    "\n",
    "Движение считается обнаруженным, если есть контуры с достаточной площадью и общее количество белых пикселей превышает min_motion_area (1000).\n",
    "Это делает детекцию более надёжной, исключая ложные срабатывания от мелких шумов.\n",
    "\n",
    "\n",
    "Информативный вывод:\n",
    "\n",
    "В сообщение о движении добавлен номер итерации (motion_counter) для ясности.\n",
    "Проверка на ошибку чтения кадра с выводом сообщения.\n",
    "\n",
    "\n",
    "\n",
    "Зачем эти улучшения\n",
    "\n",
    "Снижение шума: Морфологические операции и контурный анализ делают детекцию точнее, исключая мелкие помехи (например, дрожание света).\n",
    "Визуализация объектов: Прямоугольники вокруг движущихся объектов показывают, где конкретно происходит движение, что полезно для отладки.\n",
    "Гибкость MOG2: Настроенные параметры позволяют лучше адаптироваться к разным условиям (например, с тенями).\n",
    "Надёжность: Проверки на ошибки делают код устойчивым к сбоям камеры или видео.\n",
    "\n",
    "Как тестировать и настраивать\n",
    "\n",
    "Порог min_contour_area: Увеличьте (например, до 1000), если детектируются мелкие объекты (листья, блики). Уменьшите (до 200), если пропускаются маленькие объекты.\n",
    "Порог min_motion_area: Похож на оригинальный порог 1000. Уменьшите для большей чувствительности, увеличьте для меньшей.\n",
    "Ядро морфологии: Размер kernel (5,5) можно увеличить до (7,7) для более агрессивного подавления шума, но это может убрать мелкие объекты.\n",
    "Параметры MOG2:\n",
    "\n",
    "Увеличьте history (например, до 1000), если фон меняется медленно.\n",
    "Уменьшите varThreshold (до 10), чтобы сделать детекцию чувствительнее, или увеличьте (до 25) для меньшей чувствительности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Открытие видеопотока (0 для веб-камеры)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Проверка успешного открытия камеры\n",
    "if not cap.isOpened():\n",
    "    print(\"Ошибка: Не удалось открыть камеру.\")\n",
    "    exit()\n",
    "\n",
    "# Создание объекта MOG2 с настройками\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(\n",
    "    history=500,           # Количество кадров для модели фона\n",
    "    varThreshold=16,       # Порог для классификации пикселей\n",
    "    detectShadows=True     # Обнаружение теней\n",
    ")\n",
    "\n",
    "# Параметры морфологических операций для фильтрации шума\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "motion_counter = 0\n",
    "min_motion_area = 1000  # Порог площади движения\n",
    "min_contour_area = 500  # Минимальная площадь контура для исключения шума\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Ошибка: Не удалось захватить кадр.\")\n",
    "        break\n",
    "\n",
    "    # Применение вычитания фона\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Фильтрация шума с помощью морфологических операций\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)  # Удаление мелкого шума\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_CLOSE, kernel) # Заполнение мелких разрывов\n",
    "\n",
    "    # Подсчёт ненулевых пикселей\n",
    "    motion = cv2.countNonZero(fgmask)\n",
    "\n",
    "    # Поиск контуров для более точной детекции\n",
    "    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    motion_detected = False\n",
    "\n",
    "    # Анализ контуров\n",
    "    for contour in contours:\n",
    "        if cv2.contourArea(contour) > min_contour_area:\n",
    "            motion_detected = True\n",
    "            # Отрисовка прямоугольника вокруг движущегося объекта\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Если обнаружено движение\n",
    "    if motion_detected and motion > min_motion_area:\n",
    "        cv2.putText(\n",
    "            frame, f\"Detected motion: {motion_counter}\", (10, 40),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2\n",
    "        )\n",
    "        motion_counter += 1\n",
    "        print(f\"Сигнал: движение! (Итерация: {motion_counter})\")\n",
    "\n",
    "    # Показ кадра и маски\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('FG Mask', fgmask)\n",
    "\n",
    "    # Выход по Esc\n",
    "    if cv2.waitKey(30) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Освобождение ресурсов\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8ee76",
   "metadata": {},
   "source": [
    "2. Метод Лукаса-Канаде. Визуализировать разреженный оптический \n",
    "поток с помощью функции calcOpticalFlowPyrLK () с отрисовкой \n",
    "траектории движения. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a5bff",
   "metadata": {},
   "source": [
    "Что такое оптический поток и зачем он нужен\n",
    "Оптический поток (Optical Flow) — это метод в компьютерном зрении, который анализирует, как движутся объекты (или их части) между кадрами видео. Он не просто говорит \"есть движение\", а показывает, куда и как быстро движутся отдельные точки или объекты. Это как если бы ты смотрел на видео и видел стрелки, показывающие, в какую сторону движется каждый пиксель. Применяется в:\n",
    "\n",
    "Робототехнике: для навигации роботов или дронов (например, чтобы понять, куда движется препятствие).\n",
    "Слежение за объектами: в камерах видеонаблюдения или в спортивных приложениях (отслеживать игроков).\n",
    "Стабилизация видео: чтобы убрать дрожание камеры.\n",
    "Анализ движения: например, в играх или медицинских системах для анализа походки.\n",
    "\n",
    "Твой код использует метод Лукаса-Канаде (Lucas-Kanade) для вычисления оптического потока, отслеживая движение ключевых точек в кадрах. Он рисует траектории движения точек зелёными линиями и отмечает текущие позиции точек красными кругами. Давай разберём теорию, термины и как это работает простыми словами.\n",
    "\n",
    "Ключевые термины (терминология)\n",
    "\n",
    "Оптический поток (Optical Flow): Векторное поле, показывающее, как каждый пиксель (или группа пикселей) перемещается между кадрами. Это как \"карта движения\", где для каждой точки есть направление и скорость.\n",
    "Ключевые точки (Keypoints): Особые точки в кадре, которые легко отслеживать (например, углы, края объектов). В коде они находятся с помощью cv2.goodFeaturesToTrack.\n",
    "Метод Лукаса-Канаде (Lucas-Kanade): Алгоритм для вычисления оптического потока. Он предполагает, что движение маленькое и что яркость пикселей не меняется между кадрами. Отслеживает движение только для выбранных точек, а не всего кадра.\n",
    "goodFeaturesToTrack: Функция OpenCV, которая ищет \"хорошие\" точки для отслеживания (обычно углы, где много контраста). Это как выбрать заметные ориентиры на изображении.\n",
    "Пирамида изображений (Image Pyramid): Метод, где изображение обрабатывается в нескольких масштабах (от мелкого к крупному), чтобы лучше находить движение, даже если оно большое. В коде это задаётся через maxLevel в параметрах Лукаса-Канаде.\n",
    "Маска (Mask): В коде — это изображение того же размера, что и кадр, на котором рисуются траектории (зелёные линии). Оно \"накладывается\" на оригинальный кадр, чтобы показать движение.\n",
    "Пиксель (Pixel): Маленькая точка изображения с цветом (RGB) или яркостью (в градациях серого).\n",
    "WinSize (размер окна): Область вокруг ключевой точки, которую алгоритм анализирует для поиска движения. В коде это (15,15) — квадрат 15x15 пикселей.\n",
    "Критерии остановки (Criteria): Условия, когда алгоритм Лукаса-Канаде прекращает искать движение точки. В коде это комбинация точности (cv2.TERM_CRITERIA_EPS) и числа итераций (cv2.TERM_CRITERIA_COUNT).\n",
    "OpenCV: Библиотека для компьютерного зрения, которая делает всю работу — от чтения кадров до вычисления потока.\n",
    "\n",
    "\n",
    "Как работает метод в коде (пошагово и просто)\n",
    "Твой код реализует метод Лукаса-Канаде для отслеживания движения ключевых точек. Вот что происходит:\n",
    "\n",
    "Инициализация:\n",
    "\n",
    "Программа открывает веб-камеру (cv2.VideoCapture(0)).\n",
    "Берёт первый кадр (old_frame) и переводит его в градации серого (old_gray), так как Лукасу-Канаде не нужны цвета, только яркость.\n",
    "Находит ключевые точки с помощью cv2.goodFeaturesToTrack (до 100 углов, с параметрами качества, расстояния и размера блока). Это точки, которые легко отслеживать (например, углы объектов).\n",
    "Создаёт пустую маску (np.zeros_like(old_frame)) для рисования траекторий.\n",
    "\n",
    "\n",
    "Цикл обработки кадров:\n",
    "\n",
    "Читает новый кадр (frame) и переводит его в градации серого (frame_gray).\n",
    "Использует cv2.calcOpticalFlowPyrLK для вычисления оптического потока:\n",
    "\n",
    "Сравнивает old_gray (предыдущий кадр) и frame_gray (текущий).\n",
    "Для каждой ключевой точки (p0) ищет, куда она сместилась (p1).\n",
    "Возвращает новые координаты точек (p1), статус (st, 1 — точка найдена, 0 — потеряна) и ошибку (err).\n",
    "\n",
    "\n",
    "Если точки найдены (p1 и st не пустые):\n",
    "\n",
    "Фильтрует точки, где st == 1 (успешно отслеженные).\n",
    "Для каждой пары старой и новой точки рисует:\n",
    "\n",
    "Зелёную линию на маске (cv2.line) от старой позиции (good_old) к новой (good_new).\n",
    "Красный круг на текущем кадре (cv2.circle) в новой позиции.\n",
    "\n",
    "\n",
    "Складывает кадр с маской (cv2.add), чтобы показать и видео, и траектории.\n",
    "\n",
    "\n",
    "Показывает результат в окне (cv2.imshow).\n",
    "Обновляет предыдущий кадр (old_gray = frame_gray) и ищет новые ключевые точки (p0) для следующей итерации.\n",
    "\n",
    "\n",
    "Выход: Цикл прерывается при нажатии Esc или если видео кончилось.\n",
    "\n",
    "Как работает Лукас-Канаде внутри:\n",
    "\n",
    "Предполагает, что яркость пикселей не меняется между кадрами (если точка была яркой, она останется такой же).\n",
    "Смотрит на маленькое окно вокруг каждой ключевой точки (15x15 пикселей) и ищет, где это окно \"похоже\" в новом кадре.\n",
    "Использует пирамиду изображений (maxLevel=2), чтобы сначала искать движение на грубом уровне (низкое разрешение), а потом уточнять.\n",
    "Если точка потерялась (например, вышла за кадр), она исключается (st == 0).\n",
    "\n",
    "Почему точки выбираются заново:\n",
    "В коде p0 обновляется на каждом кадре (cv2.goodFeaturesToTrack). Это нужно, потому что точки могут \"теряться\" (выходят за кадр, закрываются объектом). Обновление помогает находить новые интересные точки.\n",
    "\n",
    "Теория: как Лукас-Канаде вычисляет движение\n",
    "Метод Лукаса-Канаде основан на двух предположениях:\n",
    "\n",
    "Постоянство яркости: Цвет или яркость точки не меняется при движении. Если угол стола был серым, он останется серым, даже если сместится.\n",
    "Малое движение: Точки двигаются не слишком далеко между кадрами (например, в пределах окна 15x15 пикселей).\n",
    "\n",
    "Алгоритм:\n",
    "\n",
    "Берёт окно вокруг ключевой точки в старом кадре.\n",
    "Ищет, где это окно \"лучше всего\" совпадает в новом кадре, минимизируя разницу в яркости.\n",
    "Использует градиенты изображения (как меняется яркость по x и y), чтобы вычислить направление и величину смещения.\n",
    "Пирамида помогает: сначала смотрит на маленькое изображение (где большие движения выглядят маленькими), потом уточняет на полном.\n",
    "\n",
    "Ограничения:\n",
    "\n",
    "Не работает, если движение слишком быстрое (точка уходит за пределы окна).\n",
    "Плохо справляется с большими изменениями освещения.\n",
    "Требует \"заметных\" точек (углов, краёв), иначе нечего отслеживать.\n",
    "\n",
    "\n",
    "Что ещё есть: альтернативы и расширения\n",
    "Есть другие подходы к анализу движения, которые могут дополнить или заменить Лукаса-Канаде:\n",
    "\n",
    "Другие методы оптического потока:\n",
    "\n",
    "Farneback: Метод в OpenCV (cv2.calcOpticalFlowFarneback). Вычисляет поток для всех пикселей, а не только ключевых точек. Даёт плотное поле (вектор для каждого пикселя), но медленнее. Подходит для анализа общего движения сцены.\n",
    "DeepFlow или FlowNet: Нейросетевые методы, которые точнее, но требуют мощного оборудования и обучения. Не в OpenCV, но есть в PyTorch/TensorFlow.\n",
    "\n",
    "\n",
    "Альтернативы без оптического потока:\n",
    "\n",
    "Вычитание фона (как в твоём первом коде с MOG2): Просто говорит, есть движение или нет, но не даёт направления.\n",
    "Контурный анализ: После потока можно найти контуры движущихся объектов (cv2.findContours) и отслеживать их.\n",
    "Трекинг объектов: Алгоритмы вроде KCF или CSRT в OpenCV (cv2.TrackerKCF_create) отслеживают конкретный объект, а не точки.\n",
    "\n",
    "\n",
    "Улучшения для твоего кода:\n",
    "\n",
    "Фильтрация точек: Использовать err (ошибку) из calcOpticalFlowPyrLK, чтобы отбрасывать точки с большой ошибкой.\n",
    "Сохранение траекторий: Сейчас маска \"накапливает\" все линии. Можно очищать её через N кадров, чтобы видеть только недавние движения.\n",
    "Анализ направления: Из векторов (good_new - good_old) можно вычислить, куда движется объект (например, \"вправо\", \"вверх\").\n",
    "Цветовая сегментация: Добавить фильтр по цвету, чтобы отслеживать только определённые объекты (например, красный мяч).\n",
    "Проблемы и решения: Если освещение меняется, можно нормализовать яркость кадров. Если движение слишком быстрое, увеличить winSize или maxLevel.\n",
    "\n",
    "\n",
    "Параметры для тюнинга:\n",
    "\n",
    "maxCorners: Больше точек — больше данных, но медленнее.\n",
    "qualityLevel: Чем выше, тем меньше, но качественнее точки.\n",
    "winSize: Большее окно лучше для быстрых движений, но теряет точность.\n",
    "maxLevel: Больше уровней пирамиды — лучше для больших движений, но дольше.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Итог\n",
    "Твой код — это классическая реализация Лукаса-Канаде для отслеживания движения ключевых точек. Он простой, но мощный для задач, где нужно понять, как движутся отдельные части сцены. Лукас-Канаде хорош для маленьких движений и заметных точек, но для сложных сцен (много объектов, быстрое движение, изменения света) могут понадобиться другие методы или доработки. Если хочешь, могу помочь настроить параметры, добавить фильтры или попробовать другой метод, например Farneback!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4996838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Параметры для выделения углов (feature detection)\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "# Параметры Лукаса-Канаде\n",
    "lk_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Получить первый кадр\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
    "\n",
    "# Маска для отрисовки траекторий\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Вычисление оптического потока\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "    if p1 is not None and st is not None:\n",
    "        good_new = p1[st==1]\n",
    "        good_old = p0[st==1]\n",
    "\n",
    "        # Отрисовка траекторий\n",
    "        for i,(new,old) in enumerate(zip(good_new, good_old)):\n",
    "            a,b = new.ravel()\n",
    "            c,d = old.ravel()\n",
    "            mask = cv2.line(mask, (int(a),int(b)), (int(c),int(d)), (0,255,0), 2)\n",
    "            frame = cv2.circle(frame, (int(a),int(b)), 5, (0,0,255), -1)\n",
    "        img = cv2.add(frame, mask)\n",
    "    else:\n",
    "        img = frame\n",
    "\n",
    "    cv2.imshow('Optical Flow (Lucas-Kanade)', img)\n",
    "    if cv2.waitKey(30) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "    old_gray = frame_gray.copy()\n",
    "    p0 = cv2.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987e8d0",
   "metadata": {},
   "source": [
    "Что улучшено в этом варианте\n",
    "\n",
    "Обработка ошибок на старте: Проверяем, удалось ли захватить первый кадр, и выходим с сообщением, если нет.\n",
    "Переинициализация точек только при необходимости: Вместо поиска новых точек каждый кадр (что замедляет и сбрасывает траектории), мы добавляем новые только если осталось меньше 20 хороших (min_points). Это делает отслеживание более стабильным.\n",
    "Фильтрация по ошибке: Используем err из calcOpticalFlowPyrLK — отбрасываем точки с ошибкой > 5 (можно настроить). Это уменьшает шум от неточных совпадений.\n",
    "Случайные цвета для траекторий: Каждая точка получает уникальный цвет (из np.random), чтобы легче различать траектории разных объектов.\n",
    "Периодическая очистка маски: Каждые 50 кадров (mask_clear_interval) маска обнуляется, чтобы старые траектории не загромождали экран вечно. Можно изменить интервал.\n",
    "Обновление p0 и colors: Только хорошие точки переносятся в следующий кадр, и цвета сохраняются.\n",
    "Обработка пустых p0: Если точек нет, ищем новые сразу, чтобы избежать ошибок.\n",
    "\n",
    "Этот код должен работать плавнее, быть визуально информативнее и меньше \"терять\" точки. Если камера низкого качества, настрой порог ошибки или min_points. Тестируй на своей машине!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fa527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Параметры для выделения углов (feature detection)\n",
    "feature_params = dict(maxCorners=100,\n",
    "                      qualityLevel=0.3,\n",
    "                      minDistance=7,\n",
    "                      blockSize=7)\n",
    "\n",
    "# Параметры Лукаса-Канаде\n",
    "lk_params = dict(winSize=(15, 15),\n",
    "                 maxLevel=2,\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Получить первый кадр\n",
    "ret, old_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Не удалось захватить первый кадр. Проверьте камеру.\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    exit()\n",
    "\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "# Маска для отрисовки траекторий\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "# Счётчик кадров для периодической очистки маски\n",
    "frame_count = 0\n",
    "mask_clear_interval = 50  # Очищать маску каждые 50 кадров, чтобы траектории не накапливались вечно\n",
    "\n",
    "# Порог для переинициализации точек (если осталось меньше, ищем новые)\n",
    "min_points = 20\n",
    "\n",
    "# Массив случайных цветов для траекторий (по одной на точку)\n",
    "if p0 is not None:\n",
    "    colors = np.random.randint(0, 255, size=(len(p0), 3))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if p0 is None or len(p0) == 0:\n",
    "        # Если точек нет, ищем новые\n",
    "        p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "        if p0 is not None:\n",
    "            colors = np.random.randint(0, 255, size=(len(p0), 3))\n",
    "        continue\n",
    "\n",
    "    # Вычисление оптического потока\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "\n",
    "    if p1 is not None and st is not None and err is not None:\n",
    "        # Фильтруем по статусу и ошибке (игнорируем точки с большой ошибкой)\n",
    "        good_mask = (st.flatten() == 1) & (err.flatten() < 5)  # Порог ошибки можно настроить\n",
    "        good_new = p1[good_mask]\n",
    "        good_old = p0[good_mask]\n",
    "        good_colors = colors[good_mask]  # Сохраняем цвета для хороших точек\n",
    "\n",
    "        # Отрисовка траекторий с разными цветами\n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            a, b = new.ravel()\n",
    "            c, d = old.ravel()\n",
    "            color = tuple(int(c) for c in good_colors[i])\n",
    "            mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color, 2)\n",
    "            frame = cv2.circle(frame, (int(a), int(b)), 5, color, -1)\n",
    "\n",
    "        img = cv2.add(frame, mask)\n",
    "\n",
    "        # Обновляем p0 и colors только хорошими точками\n",
    "        p0 = good_new.reshape(-1, 1, 2)\n",
    "        colors = good_colors\n",
    "    else:\n",
    "        img = frame\n",
    "\n",
    "    # Периодическая очистка маски\n",
    "    frame_count += 1\n",
    "    if frame_count % mask_clear_interval == 0:\n",
    "        mask = np.zeros_like(old_frame)\n",
    "\n",
    "    # Переинициализация точек, если их осталось мало\n",
    "    if len(p0) < min_points:\n",
    "        new_points = cv2.goodFeaturesToTrack(frame_gray, mask=None, **feature_params)\n",
    "        if new_points is not None:\n",
    "            p0 = np.concatenate((p0, new_points), axis=0) if len(p0) > 0 else new_points\n",
    "            new_colors = np.random.randint(0, 255, size=(len(new_points), 3))\n",
    "            colors = np.concatenate((colors, new_colors), axis=0) if len(colors) > 0 else new_colors\n",
    "\n",
    "    cv2.imshow('Optical Flow (Lucas-Kanade Improved)', img)\n",
    "    if cv2.waitKey(30) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "    old_gray = frame_gray.copy()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b48142",
   "metadata": {},
   "source": [
    "3. Применить к видео любой из трекеров отслеживания, реализованных в \n",
    "библиотеках компьютерного зрения. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3200cf56",
   "metadata": {},
   "source": [
    "\n",
    "Что такое трекинг объектов и зачем он нужен\n",
    "Трекинг объектов (Object Tracking) — это процесс в компьютерном зрении, который позволяет следить за конкретным объектом (или областью) в последовательности кадров видео. В отличие от детекции движения (как в твоём первом коде с MOG2) или оптического потока (как во втором с Лукасом-Канаде), трекинг фокусируется на отслеживании определённого объекта, который пользователь выбирает. Это полезно в:\n",
    "\n",
    "Видеонаблюдении: следить за человеком или машиной в толпе.\n",
    "Робототехнике: робот отслеживает цель (например, мяч).\n",
    "Анализе видео: отслеживать игроков в спортивных трансляциях или животных в исследованиях.\n",
    "Интерфейсах: например, отслеживать лицо для фильтров в приложениях.\n",
    "Твой код реализует трекинг с использованием нескольких алгоритмов OpenCV, позволяя пользователю выбрать объект с помощью ROI (Region of Interest, область интереса) и переключаться между разными трекерами (MIL, KCF, TLD, MOSSE, CSRT). Давай разберём теорию, термины и как это работает простыми словами.\n",
    "\n",
    "Ключевые термины (терминология)\n",
    "Трекинг объектов (Object Tracking): Процесс, при котором алгоритм запоминает выбранный объект (по его внешнему виду или особенностям) и следит за ним в новых кадрах, определяя его положение.\n",
    "ROI (Region of Interest): Прямоугольная область на кадре, которую пользователь выбирает для отслеживания. В коде это initBB — координаты и размер прямоугольника (x, y, ширина, высота).\n",
    "Трекер: Алгоритм, который отслеживает объект. OpenCV поддерживает несколько трекеров, таких как MIL, KCF, TLD, MOSSE, CSRT, каждый со своими плюсами и минусами.\n",
    "Bounding Box (BB): Прямоугольник, который окружает отслеживаемый объект. В коде это box — координаты (x, y, w, h), где w — ширина, h — высота.\n",
    "Tracker API в OpenCV: Интерфейс в OpenCV для трекинга, который включает методы init (инициализация трекера с начальным кадром и ROI) и update (обновление положения объекта в новом кадре).\n",
    "Legacy Trackers: Старые версии трекеров в OpenCV, доступные через cv2.legacy. Некоторые алгоритмы (например, MIL, TLD) в новых версиях OpenCV перемещены в legacy, чтобы поддерживать обратную совместимость.\n",
    "Кадр (Frame): Один снимок из видео, как в предыдущих кодах.\n",
    "OpenCV: Библиотека компьютерного зрения, которая предоставляет готовые трекеры и функции для работы с видео.\n",
    "Ключевые алгоритмы трекинга (в коде):\n",
    "MIL (Multiple Instance Learning): Учитывает несколько \"версий\" объекта, чтобы быть устойчивым к изменениям внешнего вида (например, поворот объекта). Хорошо работает с частичными перекрытиями, но не всегда точен.\n",
    "KCF (Kernelized Correlation Filters): Использует корреляционные фильтры для быстрого и точного трекинга. Хорошо справляется с изменениями масштаба и поворотами, но может терять объект при сильных перекрытиях.\n",
    "TLD (Tracking-Learning-Detection): Комбинирует трекинг с обучением и детекцией, чтобы находить объект, даже если он временно пропадает. Устойчив к исчезновениям, но медленнее.\n",
    "MOSSE (Minimum Output Sum of Squared Error): Очень быстрый, но менее точный. Подходит для простых задач, где объект не сильно меняется.\n",
    "CSRT (Channel and Spatial Reliability Tracking): Более точный, использует информацию о цвете и пространственную надёжность. Медленнее, но лучше справляется со сложными сценами.\n",
    "Как работает код (пошагово и просто)\n",
    "Твой код позволяет пользователю выбрать объект для отслеживания с помощью мыши (ROI) и переключать трекеры (1-5 на клавиатуре). Он использует API трекинга OpenCV. Вот как это работает:\n",
    "\n",
    "Инициализация:\n",
    "Код устанавливает зависимости (opencv-contrib-python), так как некоторые трекеры (например, CSRT) доступны только в contrib.\n",
    "Определяет словарь TRACKERS, связывающий клавиши (1-5) с названиями трекеров (MIL, KCF, TLD, MOSSE, CSRT).\n",
    "Функция create_tracker создаёт трекер по имени, сначала проверяя cv2.legacy, потом cv2, чтобы учесть разные версии OpenCV.\n",
    "По умолчанию выбирается трекер MIL (tracker_type = 'MIL').\n",
    "Открывается веб-камера (cv2.VideoCapture(0)), и initBB (начальный bounding box) пока пустой.\n",
    "Основной цикл:\n",
    "Читает кадр (ret, frame = cap.read()).\n",
    "Показывает текущий трекер на экране (cv2.putText с текстом \"Tracker: ...\").\n",
    "Если ROI уже выбран (initBB не None):\n",
    "Вызывает tracker.update(frame), чтобы найти новое положение объекта.\n",
    "Если трекинг успешен (success=True), рисует зелёный прямоугольник вокруг объекта (cv2.rectangle).\n",
    "Если трекинг провалился (success=False), показывает сообщение \"Tracking failure\".\n",
    "Показывает кадр с результатом (cv2.imshow).\n",
    "Обработка ввода пользователя:\n",
    "Клавиша 's': Запускает cv2.selectROI, чтобы пользователь мог выделить объект мышью. Если ROI выбран (не нулевой), инициализирует трекер (tracker.init(frame, initBB)).\n",
    "Клавиши 1-5: Меняют тип трекера (например, 1 — MIL, 2 — KCF). Если ROI уже есть, новый трекер инициализируется с тем же initBB.\n",
    "Esc (27): Завершает программу.\n",
    "Цикл продолжается, пока не нажат Esc или не закончено видео.\n",
    "Завершение: Закрывает камеру и окна.\n",
    "Что делает трекер внутри:\n",
    "\n",
    "При инициализации (tracker.init) трекер \"запоминает\" внешний вид объекта в ROI (цвета, текстуры, особенности).\n",
    "На каждом кадре (tracker.update) он ищет область, наиболее похожую на начальную, и возвращает новый bounding box.\n",
    "Каждый трекер использует свой алгоритм (MIL, KCF и т.д.), чтобы справляться с изменениями внешнего вида, поворотами или перекрытиями.\n",
    "Теория: как работают трекеры\n",
    "Каждый трекер решает задачу: \"где этот объект в новом кадре?\". Они отличаются подходами:\n",
    "\n",
    "MIL: Считает, что объект может немного меняться (например, из-за света). Учитывает несколько \"гипотез\" о том, как объект выглядит. Хорошо для нестабильных условий, но может \"плавать\".\n",
    "KCF: Использует корреляцию (сравнение шаблона) и машинное обучение для быстрого поиска объекта. Эффективен, но теряет объект, если он сильно закрыт.\n",
    "TLD: Комбинирует трекинг с детекцией. Если объект пропадает (например, вышел из кадра), пытается найти его снова. Подходит для длительного трекинга.\n",
    "MOSSE: Максимально быстрый, использует корреляцию с минимальной ошибкой. Хорош для простых задач, но неустойчив к изменениям.\n",
    "CSRT: Учитывает цвета и пространственную информацию, более точный, но медленный. Лучше для сложных сцен с перекрытиями.\n",
    "Общий принцип:\n",
    "\n",
    "Трекеры запоминают \"шаблон\" объекта (по пикселям или особенностям, как края).\n",
    "На каждом кадре ищут, где этот шаблон лучше всего совпадает.\n",
    "Возвращают новый bounding box или флаг, что объект потерян.\n",
    "Ограничения:\n",
    "\n",
    "Трекеры могут терять объект при сильных изменениях (освещение, поворот, перекрытие).\n",
    "Они не \"понимают\", что отслеживают (например, человек это или машина), в отличие от нейросетей вроде YOLO.\n",
    "Производительность зависит от сложности сцены и мощности компьютера.\n",
    "Что ещё есть: альтернативы и улучшения\n",
    "Другие трекеры в OpenCV:\n",
    "Boosting: Похож на MIL, но старше и медленнее. Доступен в cv2.legacy.\n",
    "MedianFlow: Быстрый, но требует, чтобы объект двигался предсказуемо. Хорош для плавных движений.\n",
    "GOTURN: Нейросетевой трекер в OpenCV (требует contrib и веса модели). Точнее, но сложнее в настройке.\n",
    "Альтернативы вне OpenCV:\n",
    "DeepSORT: Современный трекер на основе нейросетей. Используется с YOLO для точного отслеживания (например, людей в толпе).\n",
    "Siamese Networks: Нейросетевые трекеры, которые сравнивают шаблон объекта с новым кадром. Очень точные, но требуют GPU.\n",
    "Улучшения для кода:\n",
    "Сохранение траектории: Можно рисовать путь объекта (как в коде с Лукасом-Канаде) с помощью маски.\n",
    "Фильтрация по цвету: Добавить фильтр HSV, чтобы трекер фокусировался на объекте определённого цвета.\n",
    "Перезапуск при потере: Если трекинг провалился, автоматически искать объект снова (как в TLD).\n",
    "Мультитрекинг: Использовать cv2.legacy.MultiTracker для отслеживания нескольких объектов.\n",
    "Улучшение ROI: Добавить проверку, чтобы ROI не был слишком маленьким (например, if w > 10 and h > 10).\n",
    "Итог\n",
    "Твой код — это удобная демонстрация трекинга объектов с переключением алгоритмов. Он позволяет пользователю выбирать объект и тестировать разные трекеры, что полезно для сравнения их работы. MIL — хороший старт, но CSRT обычно точнее, а MOSSE — быстрее. Для реальных задач можно добавить фильтрацию, мультитрекинг или комбинировать с детекцией (например, YOLO). Если хочешь, могу предложить конкретный улучшенный код с одной из этих идей или объяснить какой-то трекер подробнее!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345212a3",
   "metadata": {},
   "source": [
    "\n",
    "## **MIL (Multiple Instance Learning)**\n",
    "\n",
    "**Как работает:**\n",
    "- Вместо одного положительного примера использует \"пакеты\" примеров\n",
    "- Для каждого кадра собирает несколько положительных образцов вокруг текущей позиции цели и отрицательные - дальше от цели\n",
    "- Обущает классификатор отличать цель от фона\n",
    "- **Ключевая идея**: даже если некоторые примеры в пакете неправильные, классификатор учится на целом наборе\n",
    "\n",
    "**Сильные стороны:**\n",
    "- Устойчив к неточностям разметки\n",
    "- Хорошо справляется с частичными перекрытиями\n",
    "- Менее чувствителен к дрожанию\n",
    "\n",
    "## **KCF (Kernelized Correlation Filters)**\n",
    "\n",
    "**Как работает:**\n",
    "1. **Корреляционные фильтры**: обучает фильтр, который дает максимальный отклик на цели и минимальный на фоне\n",
    "2. **Циклические сдвиги**: создает тренировочные примеры циклическим сдвигом исходного изображения (эффективно по памяти)\n",
    "3. **Ядерный трюк**: использует ядра (Kernel trick) для работы в высокоразмерном пространстве без явного вычисления признаков\n",
    "4. **Быстрое преобразование**: вычисления в частотной области через FFT\n",
    "\n",
    "**Сильные стороны:**\n",
    "- Невероятно быстрый (сотни FPS)\n",
    "- Точнее чем MOSSE\n",
    "- Эффективное использование памяти\n",
    "\n",
    "## **TLD (Tracking-Learning-Detection)**\n",
    "\n",
    "**Три взаимосвязанных компонента:**\n",
    "\n",
    "1. **Tracker** (слежение):\n",
    "   - Оптический поток между кадрами\n",
    "   - Предсказывает положение в следующем кадре\n",
    "\n",
    "2. **Detector** (детектирование):\n",
    "   - Каскадный классификатор по типу Viola-Jones\n",
    "   - Ищет цель по всему кадру если трекер потерял\n",
    "\n",
    "3. **Learner** (обучение):\n",
    "   - Анализирует ошибки трекера и детектора\n",
    "   - Обновляет модель цели онлайн\n",
    "   - Добавляет новые положительные/отрицательные примеры\n",
    "\n",
    "**Сильные стороны:**\n",
    "- Самовосстановление после потери цели\n",
    "- Адаптация к изменению внешнего вида\n",
    "- Очень надежный в сложных сценариях\n",
    "\n",
    "## **MOSSE (Minimum Output Sum of Squared Error)**\n",
    "\n",
    "**Как работает:**\n",
    "1. **Инициализация**: пользователь выделяет цель в первом кадре\n",
    "2. **Обучение фильтра**: находит фильтр, который минимизирует ошибку между желаемым откликом и фактическим\n",
    "3. **Свертка**: применяет обученный фильтр к новому кадру\n",
    "4. **Пик отклика**: позиция с максимальным откликом - новое положение цели\n",
    "5. **Адаптация**: постепенно обновляет фильтр для учета изменений\n",
    "\n",
    "**Сильные стороны:**\n",
    "- Экстремально быстрый (до 600 FPS)\n",
    "- Устойчив к изменениям освещения\n",
    "- Малое потребление памяти\n",
    "\n",
    "## **CSRT (Channel and Spatial Reliability Tracking)**\n",
    "\n",
    "**Как работает:**\n",
    "1. **Цветовые пространства**: использует не только HOG, но и цветовые характеристики\n",
    "2. **Пространственная надежность**: вычисляет надежность разных регионов цели\n",
    "3. **Надежность каналов**: определяет какие цветовые каналы наиболее информативны\n",
    "4. **Адаптивное окно**: автоматически подстраивает размер окна отслеживания\n",
    "\n",
    "**Сильные стороны:**\n",
    "- Высокая точность позиционирования\n",
    "- Автоматическое определение масштаба\n",
    "- Устойчив к деформациям и вращению\n",
    "\n",
    "## **Сравнение производительности:**\n",
    "- **Скорость**: MOSSE > KCF > CSRT > MIL > TLD\n",
    "- **Точность**: CSRT > TLD > KCF > MIL > MOSSE  \n",
    "- **Надежность**: TLD > CSRT > MIL > KCF > MOSSE\n",
    "\n",
    "Выбор зависит от задачи: скорость (MOSSE/KCF) vs точность (CSRT) vs надежность (TLD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44ddcc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip uninstall opencv-python opencv-contrib-python -y\n",
    "#%pip install opencv-contrib-python --no-cache-dir\n",
    "\n",
    "import cv2\n",
    "\n",
    "TRACKERS = {\n",
    "    '1': 'MIL',\n",
    "    '2': 'KCF',\n",
    "    '3': 'TLD',\n",
    "    '4': 'MOSSE',\n",
    "    '5': 'CSRT',\n",
    "}\n",
    "\n",
    "def create_tracker(tracker_type):\n",
    "    # Try legacy first\n",
    "    if hasattr(cv2.legacy, f'Tracker{tracker_type}_create'):\n",
    "        creator = getattr(cv2.legacy, f'Tracker{tracker_type}_create')\n",
    "        return creator()\n",
    "    # Fallback to direct cv2\n",
    "    elif hasattr(cv2, f'Tracker{tracker_type}_create'):\n",
    "        creator = getattr(cv2, f'Tracker{tracker_type}_create')\n",
    "        return creator()\n",
    "    raise Exception(f\"{tracker_type} tracker is not available in your OpenCV installation.\")\n",
    "\n",
    "# Default tracker\n",
    "tracker_type = 'MIL'\n",
    "tracker = create_tracker(tracker_type)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "initBB = None\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    cv2.putText(frame, f\"Tracker: {tracker_type} (Press 1-5 to change)\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "    if initBB is not None:\n",
    "        success, box = tracker.update(frame)\n",
    "        if success:\n",
    "            (x, y, w, h) = [int(v) for v in box]\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Tracking failure\", (10, 60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Tracking\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord(\"s\"):\n",
    "        initBB = cv2.selectROI(\"Tracking\", frame, fromCenter=False, showCrosshair=True)\n",
    "        if initBB:  # Only init if ROI selected (non-zero)\n",
    "            tracker = create_tracker(tracker_type)\n",
    "            tracker.init(frame, initBB)\n",
    "    elif chr(key) in TRACKERS:  # Check if key is '1' to '7'\n",
    "        new_tracker_type = TRACKERS[chr(key)]\n",
    "        if new_tracker_type != tracker_type:\n",
    "            tracker_type = new_tracker_type\n",
    "            if initBB is not None:\n",
    "                tracker = create_tracker(tracker_type)\n",
    "                tracker.init(frame, initBB)\n",
    "    elif key == 27:  # ESC to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6013a447",
   "metadata": {},
   "source": [
    "# Контрольные вопросы\n",
    "\n",
    "### 1. Принцип действия Background Subtraction Methods\n",
    "Это методы, которые строят модель \"фона\" (статической сцены), а затем на каждом кадре вычитают фон, выделяя таким образом \"новые\" (движущиеся) объекты. Пример: MOG2 моделирует фон с помощью гауссовых смесей, обновляя модель по мере поступления новых кадров.\n",
    "\n",
    "### 2. Принцип действия метода Лукаса-Канаде\n",
    "Это метод оценки оптического потока (движения яркостных пятен между кадрами). Считается, что соседние пиксели движутся одинаково (локальное постоянство), и используется аппроксимация движения на небольшом окне. Обычно применяется к выделенным ключевым точкам.\n",
    "\n",
    "### 3. В чем различие методов детектирования движения от методов отслеживания (трекеров)?\n",
    "Детектирование движения — это поиск областей, где что-то изменилось (например, появление нового объекта), а трекинг — это отслеживание уже найденного объекта на следующих кадрах, даже если его форма/размер меняются.\n",
    "\n",
    "### 4. Каков принцип работы трекеров движущихся объектов?\n",
    "Трекер получает стартовое положение объекта и далее на каждом кадре ищет его новое положение, используя алгоритмы сопоставления (по цвету, текстуре, шаблону и др.), что позволяет отслеживать даже частично перекрытые или изменившиеся объекты."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
